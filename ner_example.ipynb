{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (/Users/kirillsergeev/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n",
      "100%|██████████| 3/3 [00:00<00:00, 461.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation', 'test'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "conll = datasets.load_dataset(\"conll2003\")\n",
    "conll.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2',\n",
       " 'tokens': ['AL-AIN', ',', 'United', 'Arab', 'Emirates', '1996-12-06'],\n",
       " 'pos_tags': [22, 6, 22, 22, 23, 11],\n",
       " 'chunk_tags': [11, 0, 11, 12, 12, 12],\n",
       " 'ner_tags': [5, 0, 5, 6, 6, 0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll[\"test\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    }
   ],
   "source": [
    "CONLL_NER_TAGS = conll['train'].features['ner_tags'].feature.names\n",
    "print(CONLL_NER_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbe_vallib import NerSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = NerSampler(train = {'true': [i for i in conll['train']],\n",
    "                              'pred': [{'id': i['id'], 'ner_tags': i['ner_tags']} for i in conll['train']]},\n",
    "                     oos = {'true': [i for i in conll['test']],\n",
    "                            'pred': [{'id': i['id'], 'ner_tags': i['ner_tags']} for i in conll['test']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.set_seed(1, bootstrap=True)\n",
    "# sampler.oos['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 59.0/59.0 [00:00<00:00, 5.97kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 829/829 [00:00<00:00, 158kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 457kB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 475B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 45.2kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 433M/433M [01:30<00:00, 4.79MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import (pipeline, \n",
    "        AutoModelForTokenClassification, AutoTokenizer, \n",
    "        BertForTokenClassification, BertTokenizer)\n",
    "\n",
    "# Load pretrained model and tokenizer for English NER task (dslim/bert-base-NER)\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = BertForTokenClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "\n",
    "start_end_dct = {1:2, 3:4, 5:6, 7:8}\n",
    "\n",
    "\n",
    "def tokenize_and_preserve_tags(example: tp.Dict[str, tp.Any],\n",
    "                               tokenizer: BertTokenizer,\n",
    "                               label2id: tp.Dict[str, int],\n",
    "                               tokenizer_params={}) -> tp.Dict[str, tp.Any]:\n",
    "    # write your own function to split each pair of word-token to same number of pieces.\n",
    "\n",
    "    encoded = tokenizer(example[\"tokens\"], is_split_into_words=True, truncation=True, **tokenizer_params)\n",
    "    encoded.update(example)\n",
    "\n",
    "    labels = []\n",
    "    label = example[\"ner_tags\"]\n",
    "    # for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "    #   print(label)\n",
    "    word_ids = encoded.word_ids(batch_index=0)\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "        # ignored in the loss function.\n",
    "        if word_idx is None:\n",
    "            label_ids.append(0)\n",
    "        # We set the label for the first token of each word.\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(label[word_idx])\n",
    "        # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "        # the label_all_tokens flag.\n",
    "        else:\n",
    "            # print(label[word_idx])\n",
    "            if label[word_idx] in start_end_dct:\n",
    "              label_ids.append(start_end_dct[label[word_idx]])\n",
    "            else:\n",
    "              label_ids.append(label[word_idx])\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    label_ids[0] = 0\n",
    "    label_ids[-1] = 0\n",
    "    \n",
    "    # <YOUR CODE HERE>\n",
    "    encoded['labels'] = [label2id[CONLL_NER_TAGS[i]] for i in label_ids]\n",
    "    encoded['text_labels'] = [CONLL_NER_TAGS[i] for i in label_ids]\n",
    "    \n",
    "    assert len(encoded['labels']) == len(encoded[\"input_ids\"])\n",
    "    return encoded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"His name is Jerry Abrahamson\"\n",
    "test_example = {\"tokens\": test_sentence.split(\" \"), \"ner_tags\": [0, 0, 0, 1, 2]}\n",
    "test_result = tokenize_and_preserve_tags(test_example, tokenizer, model.config.label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1230, 1271, 1110, 5466, 7752, 2142, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1], 'tokens': ['His', 'name', 'is', 'Jerry', 'Abrahamson'], 'ner_tags': [0, 0, 0, 1, 2], 'labels': [0, 0, 0, 0, 3, 4, 4, 0], 'text_labels': ['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 3, 4, 4, 0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14041/14041 [00:04<00:00, 3205.01ex/s]\n",
      "Loading cached processed dataset at /Users/kirillsergeev/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-66f873fce4b35a3d.arrow\n",
      "Loading cached processed dataset at /Users/kirillsergeev/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-eb6b92e96444ffe1.arrow\n"
     ]
    }
   ],
   "source": [
    "conll_prepared = conll.map(lambda x: tokenize_and_preserve_tags(x, tokenizer, model.config.label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea1ba28ca3080e0412ec99e1bb1a997d4a43b70d799a956f8141a171308cbcce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
