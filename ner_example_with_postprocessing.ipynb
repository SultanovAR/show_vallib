{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (/Users/azatsultanov/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32569365eff24614afc9b3eb42471a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '2',\n",
       " 'tokens': ['AL-AIN', ',', 'United', 'Arab', 'Emirates', '1996-12-06'],\n",
       " 'pos_tags': [22, 6, 22, 22, 23, 11],\n",
       " 'chunk_tags': [11, 0, 11, 12, 12, 12],\n",
       " 'ner_tags': [5, 0, 5, 6, 6, 0]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "conll = datasets.load_dataset(\"conll2003\")\n",
    "CONLL_NER_TAGS = conll['train'].features['ner_tags'].feature.names\n",
    "print(CONLL_NER_TAGS)\n",
    "conll[\"test\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (pipeline, \n",
    "        AutoModelForTokenClassification, AutoTokenizer, \n",
    "        BertForTokenClassification, BertTokenizer)\n",
    "\n",
    "# Load pretrained model and tokenizer for English NER task (dslim/bert-base-NER)\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = BertForTokenClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.9994836,\n",
       "  'index': 9,\n",
       "  'word': 'Jerry',\n",
       "  'start': 27,\n",
       "  'end': 32},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99950564,\n",
       "  'index': 10,\n",
       "  'word': 'Ara',\n",
       "  'start': 33,\n",
       "  'end': 36},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9956499,\n",
       "  'index': 11,\n",
       "  'word': '##ham',\n",
       "  'start': 36,\n",
       "  'end': 39},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.6499991,\n",
       "  'index': 12,\n",
       "  'word': '##son',\n",
       "  'start': 39,\n",
       "  'end': 42}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = \"His name is , as we know , Jerry Arahamson\".split()\n",
    "\n",
    "NER = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=None)\n",
    "model_output = NER(\" \".join(tokens))\n",
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER', 'word': 'Jerry', 'start': 27, 'end': 32},\n",
       " {'entity': 'I-PER', 'word': 'Arahamson', 'start': 33, 'end': 42}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unite_entities(entities):\n",
    "    if len(entities) <= 1:\n",
    "        return entities\n",
    "\n",
    "    united_result = []\n",
    "    cur_entity = {key: entities[0][key] for key in ['entity', 'word', 'start', 'end']}\n",
    "    for entity in entities[1:]:\n",
    "        if entity['word'].startswith('##'):\n",
    "            cur_entity['word'] += entity['word'].lstrip('#')\n",
    "            cur_entity['end'] = entity['end']\n",
    "        else:\n",
    "            united_result.append(cur_entity)\n",
    "            cur_entity = {key: entity[key] for key in ['entity', 'word', 'start', 'end']}\n",
    "    united_result.append(cur_entity)\n",
    "    return united_result\n",
    "\n",
    "entities = unite_entities(model_output)\n",
    "entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_entities_to_bio(tokens, entities):\n",
    "    bio_tags = []\n",
    "\n",
    "    cur_entity_idx = 0\n",
    "    for token in tokens:\n",
    "        if token == entities[cur_entity_idx]['word']:\n",
    "            bio_tags.append(entities[cur_entity_idx]['entity'])\n",
    "            cur_entity_idx += 1\n",
    "        else:\n",
    "            bio_tags.append('O')\n",
    "    return bio_tags\n",
    "\n",
    "bio_tags = convert_entities_to_bio(tokens, entities)\n",
    "bio_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def postprocessing_model(model_output):\n",
    "    entities = unite_entities(model_output)\n",
    "    bio_tags = convert_entities_to_bio(tokens, entities)\n",
    "    return bio_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.training import biluo_tags_to_offsets, offsets_to_biluo_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets_to_biluo_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-PER', 'I-PER']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E177] Ill-formed IOB input detected: B",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m words \u001b[39m=\u001b[39m sample[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(iob_tags)\n\u001b[0;32m---> 10\u001b[0m biluo_tags \u001b[39m=\u001b[39m [iob_to_biluo(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m iob_tags]\n\u001b[1;32m     12\u001b[0m doc \u001b[39m=\u001b[39m Doc(Vocab(), words\u001b[39m=\u001b[39mwords)\n\u001b[1;32m     13\u001b[0m biluo_tags_to_offsets(doc, biluo_tags)\n",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m words \u001b[39m=\u001b[39m sample[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(iob_tags)\n\u001b[0;32m---> 10\u001b[0m biluo_tags \u001b[39m=\u001b[39m [iob_to_biluo(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m iob_tags]\n\u001b[1;32m     12\u001b[0m doc \u001b[39m=\u001b[39m Doc(Vocab(), words\u001b[39m=\u001b[39mwords)\n\u001b[1;32m     13\u001b[0m biluo_tags_to_offsets(doc, biluo_tags)\n",
      "File \u001b[0;32m~/Programming/vallib/vallib_env/lib/python3.8/site-packages/spacy/training/iob_utils.py:13\u001b[0m, in \u001b[0;36miob_to_biluo\u001b[0;34m(tags)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mwhile\u001b[39;00m tags:\n\u001b[1;32m     12\u001b[0m     out\u001b[39m.\u001b[39mextend(_consume_os(tags))\n\u001b[0;32m---> 13\u001b[0m     out\u001b[39m.\u001b[39mextend(_consume_ent(tags))\n\u001b[1;32m     14\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Programming/vallib/vallib_env/lib/python3.8/site-packages/spacy/training/iob_utils.py:46\u001b[0m, in \u001b[0;36m_consume_ent\u001b[0;34m(tags)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m length \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(label) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 46\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE177\u001b[39m.\u001b[39mformat(tag\u001b[39m=\u001b[39mtag))\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mU-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m label]\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: [E177] Ill-formed IOB input detected: B"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "sample = conll['train'][1]\n",
    "\n",
    "iob_tags = list(map(lambda x: CONLL_NER_TAGS[x], sample['ner_tags']))\n",
    "words = sample['tokens']\n",
    "\n",
    "print(iob_tags)\n",
    "\n",
    "biluo_tags = [iob_to_biluo(i) for i in iob_tags]\n",
    "\n",
    "doc = Doc(Vocab(), words=words)\n",
    "biluo_tags_to_offsets(doc, biluo_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biluo_tags_to_offsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "import torch\n",
    "\n",
    "start_end_dct = {1:2, 3:4, 5:6, 7:8}\n",
    "\n",
    "\n",
    "def tokenize_and_preserve_tags(example: tp.Dict[str, tp.Any],\n",
    "                               tokenizer: BertTokenizer,\n",
    "                               label2id: tp.Dict[str, int],\n",
    "                               tokenizer_params={}) -> tp.Dict[str, tp.Any]:\n",
    "    # write your own function to split each pair of word-token to same number of pieces.\n",
    "\n",
    "    encoded = tokenizer(example[\"tokens\"], is_split_into_words=True, truncation=True, **tokenizer_params)\n",
    "    encoded.update(example)\n",
    "\n",
    "    labels = []\n",
    "    label = example[\"ner_tags\"]\n",
    "    # for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "    #   print(label)\n",
    "    word_ids = encoded.word_ids(batch_index=0)\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "        # ignored in the loss function.\n",
    "        if word_idx is None:\n",
    "            label_ids.append(0)\n",
    "        # We set the label for the first token of each word.\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(label[word_idx])\n",
    "        # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "        # the label_all_tokens flag.\n",
    "        else:\n",
    "            # print(label[word_idx])\n",
    "            if label[word_idx] in start_end_dct:\n",
    "              label_ids.append(start_end_dct[label[word_idx]])\n",
    "            else:\n",
    "              label_ids.append(label[word_idx])\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    label_ids[0] = 0\n",
    "    label_ids[-1] = 0\n",
    "    \n",
    "    # <YOUR CODE HERE>\n",
    "    encoded['labels'] = [label2id[CONLL_NER_TAGS[i]] for i in label_ids]\n",
    "    encoded['text_labels'] = [CONLL_NER_TAGS[i] for i in label_ids]\n",
    "    \n",
    "    # assert len(encoded['labels']) == len(encoded[\"input_ids\"])\n",
    "    return encoded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1230, 1271, 1110, 5466, 7752, 2142,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]]), 'tokens': ['His', 'name', 'is', 'Jerry', 'Abrahamson'], 'ner_tags': [0, 0, 0, 1, 2], 'labels': [0, 0, 0, 0, 3, 4, 4, 0], 'text_labels': ['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"His name is Jerry Abrahamson\"\n",
    "test_example = {\"tokens\": test_sentence.split(\" \"), \"ner_tags\": [0, 0, 0, 1, 2]}\n",
    "test_result = tokenize_and_preserve_tags(test_example, tokenizer, model.config.label2id, {'return_tensors': 'pt'})\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/azatsultanov/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-574d598a9a4387b0.arrow\n",
      "Loading cached processed dataset at /Users/azatsultanov/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-293a8d4b2cdbd114.arrow\n",
      "Loading cached processed dataset at /Users/azatsultanov/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-9d19ba3e83272a3d.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '1',\n",
       " 'tokens': ['Peter', 'Blackburn'],\n",
       " 'pos_tags': [22, 22],\n",
       " 'chunk_tags': [11, 12],\n",
       " 'ner_tags': [1, 2],\n",
       " 'input_ids': [[101, 1943, 14428, 102]],\n",
       " 'token_type_ids': [[0, 0, 0, 0]],\n",
       " 'attention_mask': [[1, 1, 1, 1]],\n",
       " 'labels': [0, 3, 4, 0],\n",
       " 'text_labels': ['O', 'B-PER', 'I-PER', 'O']}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_prepared = conll.map(lambda x: tokenize_and_preserve_tags(x, tokenizer, model.config.label2id, tokenizer_params={'return_tensors': 'pt'}))\n",
    "conll_prepared['train'][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_dataset(dataset):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    for i in dataset:\n",
    "        tokens.append(i['tokens'])\n",
    "        labels.append(i['text_labels'])\n",
    "        \n",
    "    return {'X': tokens, 'y_true': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbe_vallib import NerSampler\n",
    "\n",
    "sampler = NerSampler(train=preprocessing_dataset(conll_prepared['train']),\n",
    "                     oos=preprocessing_dataset(conll_prepared['test']))\n",
    "sampler.set_seed(1, bootstrap=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ModelWrapper():\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.id2label = model.config.id2label\n",
    "        self.NER_hugface = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy='first')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            encoded = self.tokenizer(X, is_split_into_words=True,\n",
    "                                    truncation=True,\n",
    "                                    padding=True,\n",
    "                                    return_tensors='pt')\n",
    "            model_output = model(**encoded)\n",
    "            print(model_output['offset_mapping'])\n",
    "            ner_entities = self.NER_hugface.postprocess(model_output)\n",
    "            return ner_entities\n",
    "    \n",
    "    # def _logit2label(self, logits):\n",
    "    #     idxs = torch.argmax(logits, dim=-1)\n",
    "    #     unique_vals, inv_idxs = torch.unique(idxs, return_inverse=True, dim=-1)\n",
    "    #     unique_vals = np.array([model.config.id2label[int(i)] for i in unique_vals[0]])\n",
    "    #     labels = unique_vals[inv_idxs].reshape(idxs.shape)\n",
    "    #     return labels\n",
    "\n",
    "\n",
    "wrapped_model = ModelWrapper(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_output = tokenizer(['Lyuis', 'Arbrams'], is_split_into_words=True,\n",
    "                                    truncation=True,\n",
    "                                    padding=True,\n",
    "                                    return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 9])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**tok_output).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vallib_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4cc85873d42337b9e9e8da282a699b0819447065cfa6c32ea38d6a740a63d56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
